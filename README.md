
### An untrained transformer based on the "Attention is all you need" paper 
### A Character level Bigram and MLP model  
### Nanograd 
### Reciprocal rank fusion algo for RAG 

## Note:
### Training and testing datasets are yet to be created and will take time 
